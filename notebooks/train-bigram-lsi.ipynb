{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     c:\\Users\\user\\anaconda3\\lib\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string, STOPWORDS, remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_tags, stem_text, strip_multiple_whitespaces \n",
    "from gensim.parsing.preprocessing import strip_non_alphanum, strip_punctuation\n",
    "from gensim.models.phrases import Phraser, Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.utils import tokenize\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.similarities import Similarity\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from bs4 import BeautifulSoup as BSHTML\n",
    "\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../data/cleaned_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>original_category</th>\n",
       "      <th>category_list</th>\n",
       "      <th>category_string</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>title</th>\n",
       "      <th>4_features_combined</th>\n",
       "      <th>5_features_combined</th>\n",
       "      <th>4_features_tokenized</th>\n",
       "      <th>5_features_tokenized</th>\n",
       "      <th>category_group_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0011300000</td>\n",
       "      <td>[Electronics, Camera &amp;amp; Photo, Video Survei...</td>\n",
       "      <td>[Electronics, Camera &amp; Photo, Video Surveillan...</td>\n",
       "      <td>Electronics, Camera &amp; Photo, Video Surveillanc...</td>\n",
       "      <td>Camera &amp; Photo</td>\n",
       "      <td>Genuine Geovision 1 Channel 3rd Party NVR IP S...</td>\n",
       "      <td>Genuine Geovision 1 Channel 3rd Party NVR IP S...</td>\n",
       "      <td>Genuine Geovision 1 Channel 3rd Party NVR IP S...</td>\n",
       "      <td>[genuin, geovis, channel, parti, nvr, softwar,...</td>\n",
       "      <td>[genuin, geovis, channel, parti, nvr, softwar,...</td>\n",
       "      <td>1392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0043396828</td>\n",
       "      <td>[Electronics, Camera &amp;amp; Photo]</td>\n",
       "      <td>[Electronics, Camera &amp; Photo]</td>\n",
       "      <td>Electronics, Camera &amp; Photo</td>\n",
       "      <td>Camera &amp; Photo</td>\n",
       "      <td>Books \"Handbook of Astronomical Image Processi...</td>\n",
       "      <td>Books \"Handbook of Astronomical Image Processi...</td>\n",
       "      <td>Books \"Handbook of Astronomical Image Processi...</td>\n",
       "      <td>[book, handbook, astronom, imag, process, rom,...</td>\n",
       "      <td>[book, handbook, astronom, imag, process, rom,...</td>\n",
       "      <td>5779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060009810</td>\n",
       "      <td>[Electronics, eBook Readers &amp;amp; Accessories,...</td>\n",
       "      <td>[Electronics, eBook Readers &amp; Accessories, eBo...</td>\n",
       "      <td>Electronics, eBook Readers &amp; Accessories, eBoo...</td>\n",
       "      <td>Books</td>\n",
       "      <td>One Hot Summer</td>\n",
       "      <td>One Hot Summer Electronics, eBook Readers &amp; Ac...</td>\n",
       "      <td>One Hot Summer Electronics, eBook Readers &amp; Ac...</td>\n",
       "      <td>[hot, summer, electron, ebook, reader, accesso...</td>\n",
       "      <td>[hot, summer, electron, ebook, reader, accesso...</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0060219602</td>\n",
       "      <td>[Electronics, eBook Readers &amp; Accessories, eBo...</td>\n",
       "      <td>[Electronics, eBook Readers &amp; Accessories, eBo...</td>\n",
       "      <td>Electronics, eBook Readers &amp; Accessories, eBoo...</td>\n",
       "      <td>Books</td>\n",
       "      <td>Hurray for Hattie Rabbit: Story and pictures (...</td>\n",
       "      <td>Hurray for Hattie Rabbit: Story and pictures (...</td>\n",
       "      <td>Hurray for Hattie Rabbit: Story and pictures (...</td>\n",
       "      <td>[hurrai, hatti, rabbit, stori, pictur, earli, ...</td>\n",
       "      <td>[hurrai, hatti, rabbit, stori, pictur, earli, ...</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0060786817</td>\n",
       "      <td>[Electronics, eBook Readers &amp; Accessories, eBo...</td>\n",
       "      <td>[Electronics, eBook Readers &amp; Accessories, eBo...</td>\n",
       "      <td>Electronics, eBook Readers &amp; Accessories, eBoo...</td>\n",
       "      <td>Books</td>\n",
       "      <td>sex.lies.murder.fame.: A Novel</td>\n",
       "      <td>sex.lies.murder.fame.: A Novel Electronics, eB...</td>\n",
       "      <td>sex.lies.murder.fame.: A Novel Electronics, eB...</td>\n",
       "      <td>[sex, li, murder, fame, novel, electron, ebook...</td>\n",
       "      <td>[sex, li, murder, fame, novel, electron, ebook...</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                  original_category  \\\n",
       "0  0011300000  [Electronics, Camera &amp; Photo, Video Survei...   \n",
       "1  0043396828                  [Electronics, Camera &amp; Photo]   \n",
       "2  0060009810  [Electronics, eBook Readers &amp; Accessories,...   \n",
       "3  0060219602  [Electronics, eBook Readers & Accessories, eBo...   \n",
       "4  0060786817  [Electronics, eBook Readers & Accessories, eBo...   \n",
       "\n",
       "                                       category_list  \\\n",
       "0  [Electronics, Camera & Photo, Video Surveillan...   \n",
       "1                      [Electronics, Camera & Photo]   \n",
       "2  [Electronics, eBook Readers & Accessories, eBo...   \n",
       "3  [Electronics, eBook Readers & Accessories, eBo...   \n",
       "4  [Electronics, eBook Readers & Accessories, eBo...   \n",
       "\n",
       "                                     category_string        main_cat  \\\n",
       "0  Electronics, Camera & Photo, Video Surveillanc...  Camera & Photo   \n",
       "1                        Electronics, Camera & Photo  Camera & Photo   \n",
       "2  Electronics, eBook Readers & Accessories, eBoo...           Books   \n",
       "3  Electronics, eBook Readers & Accessories, eBoo...           Books   \n",
       "4  Electronics, eBook Readers & Accessories, eBoo...           Books   \n",
       "\n",
       "                                               title  \\\n",
       "0  Genuine Geovision 1 Channel 3rd Party NVR IP S...   \n",
       "1  Books \"Handbook of Astronomical Image Processi...   \n",
       "2                                     One Hot Summer   \n",
       "3  Hurray for Hattie Rabbit: Story and pictures (...   \n",
       "4                     sex.lies.murder.fame.: A Novel   \n",
       "\n",
       "                                 4_features_combined  \\\n",
       "0  Genuine Geovision 1 Channel 3rd Party NVR IP S...   \n",
       "1  Books \"Handbook of Astronomical Image Processi...   \n",
       "2  One Hot Summer Electronics, eBook Readers & Ac...   \n",
       "3  Hurray for Hattie Rabbit: Story and pictures (...   \n",
       "4  sex.lies.murder.fame.: A Novel Electronics, eB...   \n",
       "\n",
       "                                 5_features_combined  \\\n",
       "0  Genuine Geovision 1 Channel 3rd Party NVR IP S...   \n",
       "1  Books \"Handbook of Astronomical Image Processi...   \n",
       "2  One Hot Summer Electronics, eBook Readers & Ac...   \n",
       "3  Hurray for Hattie Rabbit: Story and pictures (...   \n",
       "4  sex.lies.murder.fame.: A Novel Electronics, eB...   \n",
       "\n",
       "                                4_features_tokenized  \\\n",
       "0  [genuin, geovis, channel, parti, nvr, softwar,...   \n",
       "1  [book, handbook, astronom, imag, process, rom,...   \n",
       "2  [hot, summer, electron, ebook, reader, accesso...   \n",
       "3  [hurrai, hatti, rabbit, stori, pictur, earli, ...   \n",
       "4  [sex, li, murder, fame, novel, electron, ebook...   \n",
       "\n",
       "                                5_features_tokenized  category_group_count  \n",
       "0  [genuin, geovis, channel, parti, nvr, softwar,...                  1392  \n",
       "1  [book, handbook, astronom, imag, process, rom,...                  5779  \n",
       "2  [hot, summer, electron, ebook, reader, accesso...                   399  \n",
       "3  [hurrai, hatti, rabbit, stori, pictur, earli, ...                   399  \n",
       "4  [sex, li, murder, fame, novel, electron, ebook...                   399  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folders_to_save_to_file(file):\n",
    "    filepath = \"/\".join(file.split(\"/\")[:-1])\n",
    "    if filepath:\n",
    "        os.makedirs(filepath, exist_ok=True)\n",
    "\n",
    "def train_lsi(df, num_topics, feature: str, save_model: str=\"../data/recommend/lsi_index/lsi_index_bigram\"):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    - df: Pandas DataFrame\n",
    "    - feature: str value of the dataframe's column\n",
    "    - save_model: \n",
    "    \"\"\"\n",
    "    tokenized = []\n",
    "\n",
    "    bigram_model = Phrases(df[feature], connector_words = ENGLISH_CONNECTOR_WORDS)\n",
    "\n",
    "    for text in tqdm(df[feature]):\n",
    "\n",
    "        # apply the bigram model to the lemmatized text.\n",
    "        # if applied correctly, bigrammed_tokens contains a list of unigrams and bigrams\n",
    "        # generated from the lemmatized tokens\n",
    "        bigrammed_tokens = bigram_model[text]\n",
    "\n",
    "        # append to tokenized list\n",
    "        tokenized.append(bigrammed_tokens)\n",
    "\n",
    "    dictionary = corpora.Dictionary(tokenized)\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized]\n",
    "    # word_counts = [[(dictionary[id], count) for id, count in line] for line in corpus]\n",
    "    lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=num_topics)\n",
    "\n",
    "    #index = similarities.MatrixSimilarity(lsi[corpus])  # transform corpus to LSI space and index it\n",
    "    create_folders_to_save_to_file(save_model)\n",
    "    create_folders_to_save_to_file('../data/recommend/lsi_dictionary/lsi_dict_bigram')\n",
    "    create_folders_to_save_to_file('../data/recommend/lsi_model/lsi_model_bigram')\n",
    "    create_folders_to_save_to_file('../data/recommend/lsi_trained_bigram_model/trained_lsi_bigram_model')\n",
    "    index = Similarity(save_model, lsi[corpus], num_features=len(dictionary))  # transform corpus to LSI space and index it\n",
    "    # index = Similarity(save_model, lsi[corpus], num_features=num_topics)  # transform corpus to LSI space and index it\n",
    "    dictionary.save('../data/recommend/lsi_dictionary/lsi_dict_bigram')\n",
    "    lsi.save('../data/recommend/lsi_model/lsi_model_bigram')\n",
    "    index.save(save_model)\n",
    "    bigram_model.save('../data/recommend/lsi_trained_bigram_model/trained_lsi_bigram_model')\n",
    "    return lsi, index, dictionary, bigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f705e43e389a4a239f5cc5f9815be977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/786445 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create model\n",
    "lsi, index, dictionary, lsi_bigram_model = train_lsi(df, num_topics=25, feature=\"4_features_tokenized\", save_model=\"../data/recommend/lsi_index/lsi_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_categories(query, bigram_model, lsi_model, lsi_index, lsi_dictionary):\n",
    "    # initialize Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # use gensim preprocessing filters \n",
    "    custom_filters = [lambda x: x.lower(), \n",
    "                      strip_tags, \n",
    "                      strip_punctuation, \n",
    "                      strip_multiple_whitespaces, \n",
    "                      strip_non_alphanum,\n",
    "                      remove_stopwords]\n",
    "\n",
    "    # tokenize, strip tags, punctuation, multi_whitespace, non_alphanum, and remove stopwords\n",
    "    tokenized_query = preprocess_string(query, custom_filters)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmatized_query = [lemmatizer.lemmatize(word) for word in tokenized_query]\n",
    "\n",
    "    # apply the bigram model to the lemmatized text.\n",
    "    # if applied correctly, bigrammed_tokens contains a list of unigrams and bigrams\n",
    "    # generated from the lemmatized tokens\n",
    "    bigrammed_query = bigram_model[lemmatized_query]\n",
    "    \n",
    "    vec_bow = dictionary.doc2bow(bigrammed_query)\n",
    "    vec_lsi = lsi[vec_bow]  # convert the query to LSI space\n",
    "    print('vec_lsi:', vec_lsi)\n",
    "    print(\"======================\")\n",
    "    sims = index[vec_lsi]  # perform a similarity query against the corpus\n",
    "    #print(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples\n",
    "\n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "    for doc_position, doc_score in sims[:10]:\n",
    "        print(doc_score, df.category_list[doc_position])\n",
    "        print(df.title[doc_position])\n",
    "        print(\"======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec_lsi: [(0, 0.028112193945198503), (1, -0.004929436654364791), (2, -0.08840807037051494), (3, -0.08636846081939248), (4, 0.012289085389350706), (5, -0.0240277300689112), (6, 0.11517578338712182), (7, 0.09660060956920022), (8, -0.11937939008714527), (9, -0.14937192974391741), (10, 0.04276714157336668), (11, 0.01093153732734963), (12, 0.2761933901486182), (13, 0.046466561694928105), (14, 0.10720872830710111), (15, -0.37333136070166006), (16, -0.03894470516760002), (17, 0.03776076499576643), (18, 0.06985652245435413), (19, -0.08094858637011085), (20, 0.19875782605740835), (21, 0.2715270252404035), (22, 0.060742546580165875), (23, -0.39267521398962735), (24, 0.35013350092131257)]\n",
      "======================\n",
      "0.9118724 ['Electronics', 'Home Audio', 'Speakers', 'Outdoor Speakers']\n",
      "Pa Horn Weatherproof Abs Speaker PA Loudspeaker Speaker Horn\n",
      "======================\n",
      "0.9105098 ['Electronics', 'Home Audio', 'Speakers', 'Ceiling & In-Wall Speakers']\n",
      "8 Ceiling Wall Mount Speakers - Pair of 2-Way Midbass Woofer Speaker Directable 1 Titanium Dome Tweeter Flush Design w/ 55Hz-22kHz Frequency Response & 300 Watts Peak Easy Installation - Pyle PDIC80\n",
      "======================\n",
      "0.9104252 ['Electronics', 'Home Audio', 'Speakers', 'Ceiling & In-Wall Speakers']\n",
      "6.5&rdquo; Ceiling Wall Mount Speakers - Pair of 2-Way Midbass Woofer Speaker 1/2'' Polymer Dome Tweeter Flush Design w/ 70Hz-20kHz Frequency Response & 200 Watts Peak Easy Installation - Pyle PDIC61RD\n",
      "======================\n",
      "0.89959913 ['Electronics', 'Home Audio', 'Speakers', 'Outdoor Speakers']\n",
      "ION Planter Speaker Wireless Outdoor Speaker with Weather-Resistant Design and Integrated Drainage - Speakers - Retail Packaging - Clay\n",
      "======================\n",
      "0.87734133 ['Electronics', 'Portable Audio & Video', 'Portable Speakers & Docks', 'Portable Bluetooth Speakers', 'Imported', '#BlackFriday, best outdoor waterproof bluetooth speakers, best waterproof bluetooth speakers 2013, best waterproof bluetooth speakers 2014, black friday, Blackberry, bluetooth speaker, bluetooth waterproof speaker and speakerphone, bluetooth waterproof speakers pool, boat speaker, braven waterproof bluetooth speaker review, brookstone waterproof bluetooth speaker manual, brookstone waterproof bluetooth speaker review']\n",
      "Sound Monkey Audio - High Performance Waterproof Dustproof Bluetooth 10 Watt Speaker\n",
      "======================\n",
      "0.87734133 ['Electronics', 'Portable Audio & Video', 'Portable Speakers & Docks', 'Portable Bluetooth Speakers', 'Imported', '#BlackFriday, best outdoor waterproof bluetooth speakers, best waterproof bluetooth speakers 2013, best waterproof bluetooth speakers 2014, black friday, Blackberry, bluetooth speaker, bluetooth waterproof speaker and speakerphone, bluetooth waterproof speakers pool, boat speaker, braven waterproof bluetooth speaker review, brookstone waterproof bluetooth speaker manual, brookstone waterproof bluetooth speaker review']\n",
      "Sound Monkey Audio - High Performance Waterproof Dustproof Bluetooth 10 Watt Speaker\n",
      "======================\n",
      "0.8747173 ['Electronics', 'Home Audio', 'Speakers', 'Outdoor Speakers']\n",
      "SCS ETC Portable Wireless Bluetooth Speaker with 7000mAh Power Bank-3D Surround Sound Outdoor Sports Speakers-Black\n",
      "======================\n",
      "0.87017405 ['Electronics', 'Home Audio', 'Speakers', 'Outdoor Speakers']\n",
      "JBL Control 29AV-1 Premium Indoor / Outdoor Monitor Speaker (Black)\n",
      "======================\n",
      "0.8676828 ['Electronics', 'Home Audio', 'Speakers', 'Outdoor Speakers']\n",
      "Infinity OUTRIGGER JR Outdoor Indoor 2-Way Speaker 1 PAIR\n",
      "======================\n",
      "0.86679333 ['Electronics', 'Home Audio', 'Speakers']\n",
      "JB. LAB HRS-32PB High Resolution Mini Bluetooth Speaker - Portable Bluetooth Speaker - Wireless Hi-fi Speaker - Pocket Size Bluetooth Speaker - Black\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "query_term = \"computer speakers with microphone\"\n",
    "# query_term = \"i want a kingston sd card\"\n",
    "# query_term = \"i want a computer monitor\"\n",
    "\n",
    "return_categories(query_term, bigram_model=lsi_bigram_model, lsi_model=lsi, lsi_index=index, lsi_dictionary=dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
