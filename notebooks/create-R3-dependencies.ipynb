{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import scipy\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from rake_nltk import Rake\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import islice\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string, STOPWORDS, remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_tags, stem_text, strip_multiple_whitespaces \n",
    "from gensim.parsing.preprocessing import strip_non_alphanum, strip_punctuation\n",
    "from gensim.models.phrases import Phraser, Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.utils import tokenize\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LsiModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_dict = joblib.load('../data/recommend/product_dict.joblib')\n",
    "index_asin = pd.read_pickle('../data/recommend/index_asin.pickle')\n",
    "category_index = joblib.load('../data/recommend/category_index.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list = pd.read_csv('../data/recommend/keyword_list_1000.csv')\n",
    "keyword_list = keyword_list.keywords.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_dictionary = pd.read_pickle('../data/recommend/lsi_dictionary_10_components/lsi_dict_bigram')\n",
    "lsi_model = LsiModel.load('../data/recommend/lsi_model_10_components/lsi_model_bigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_bigram_model = joblib.load('../data/recommend/reviews_bigram_model.joblib')\n",
    "lsi_bigram_model = joblib.load('../data/recommend/lsi_trained_bigram_model_10_components/trained_lsi_bigram_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_keyword_master_index = joblib.load('../data/recommend/v2/review_keyword_master_index.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_reviewer = joblib.load('../data/recommend/index_reviewer.joblib')\n",
    "reviewer_index = joblib.load('../data/recommend/reviewer_index.joblib')\n",
    "index_product = joblib.load('../data/recommend/index_product.joblib')\n",
    "product_index = joblib.load('../data/recommend/product_index.joblib')\n",
    "common_product_dict = joblib.load('../data/recommend/common_product_dict.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Products Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_products_dict(file_path, chunksize):\n",
    "    '''\n",
    "    Extracts features from the Amazon products dataset.\n",
    "\n",
    "    Args:\n",
    "        file_path: file path to product dataset, should be in json format\n",
    "        chunksize: size of the chunks to read in json\n",
    "\n",
    "    Returns:\n",
    "        product_dict: dictionary where key is product asin and values is a dictionary of product title, category, price, and num_features\n",
    "    '''\n",
    "    iter_df = pd.read_json(file_path, lines=True, chunksize=chunksize)\n",
    "\n",
    "    product_dict = dict()\n",
    "\n",
    "    for chunk in iter_df:\n",
    "        for idx, row in chunk.iterrows():\n",
    "            asin = row['asin']\n",
    "            title = row['title']\n",
    "            category = row['category']\n",
    "            price = row['price']\n",
    "            num_features = len(row['feature'])\n",
    "\n",
    "            # clean the category\n",
    "            category = [cat.strip().replace('&amp;', '&') for cat in category]\n",
    "\n",
    "            # clean price\n",
    "            try:\n",
    "                if price == '':\n",
    "                    price = 0\n",
    "                elif len(price) > 7:\n",
    "                    price = 0\n",
    "                else:\n",
    "                    price = price.replace('$', '')\n",
    "                    price = price.replace(',', '')\n",
    "                    price = float(price)\n",
    "            except:\n",
    "                print(asin, ':', price)\n",
    "\n",
    "            attr_dict = {'title' : title, 'category' : category, 'price' : price, 'num_features' : num_features}\n",
    "            \n",
    "            if asin not in product_dict:\n",
    "                product_dict[asin] = attr_dict\n",
    "            else:\n",
    "                product_dict[asin].update(attr_dict)\n",
    "\n",
    "    \n",
    "    return product_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_dict = create_products_dict('../data/meta_Electronics.json', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(product_dict, '../data/recommend/product_dict.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Master Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_reviews_to_tokens(text, lsi_bigram_model, lsi_model, lsi_dictionary):\n",
    "    '''\n",
    "    Converts review text into bigrammed tokens. Performs filtering of tags, punctuation, whitespaces,\n",
    "    alpha numerica, and stopwords before lemmatizing.\n",
    "\n",
    "    Args:\n",
    "        text: a single review text in string format\n",
    "        lsi_bigram_model: gensim bigram model\n",
    "        lsi_model: gensim lsi model\n",
    "        lsi_dictionary: gensim lsi dictionary\n",
    "\n",
    "    Returns:\n",
    "        bigrammed_text: list of tokens that have been lemmatized and run through the bigram model.\n",
    "    '''\n",
    "    # initialize Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # use gensim preprocessing filters \n",
    "    custom_filters = [lambda x: x.lower(), \n",
    "                      strip_tags, \n",
    "                      strip_punctuation, \n",
    "                      strip_multiple_whitespaces, \n",
    "                      strip_non_alphanum,\n",
    "                      remove_stopwords]\n",
    "\n",
    "    # tokenize, strip tags, punctuation, multi_whitespace, non_alphanum, and remove stopwords\n",
    "    tokenized_text = preprocess_string(text, custom_filters)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in tokenized_text]\n",
    "\n",
    "    # apply the bigram model to the lemmatized text.\n",
    "    # if applied correctly, bigrammed_tokens contains a list of unigrams and bigrams\n",
    "    # generated from the lemmatized tokens\n",
    "    bigrammed_text = lsi_bigram_model[lemmatized_text]\n",
    "    \n",
    "    return bigrammed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_text_score(text):\n",
    "    word_set_good = (['good_quality', 'great_quality', 'high_quality', 'excellent_quality','quality_product', 'nice_quality',\n",
    "                     'better_quality', 'best_quality', 'highest_quality', 'amazing_quality', 'awesome_quality', 'top_quality',\n",
    "                     'quality_item', 'quality_made', 'quality_material', 'works_great', 'works_well', 'works_perfectly',\n",
    "                     'worked_great', 'work_great', 'work_well', 'worked_perfectly','works_flawlessly', 'worked_flawlessly',\n",
    "                     'worked_perfect', 'working_perfectly', 'works_wonderfully', 'works_amazingly', 'work_wonderfully',\n",
    "                     'work_excellent', 'work_awesome', 'worked_excellent','quality_construction', 'built_quality', \n",
    "                      'fantastic_quality', 'perfect_quality', 'superior_quality'])\n",
    "    \n",
    "    word_set_bad = ['poor_quality', 'low_quality', 'cheap_quality','bad_quality', 'quality_control', 'poor_build', \n",
    "                    'stopped_working', 'stop_working', 'never_worked', 'quit_working', 'nothing_works', 'stop_working' ]\n",
    "    score = 0 \n",
    "    for token in text:\n",
    "        if token in word_set_good:\n",
    "            score = score + 1\n",
    "\n",
    "        if token in word_set_bad:\n",
    "            score = score - 1\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_text_score(text):\n",
    "    word_set_bad = (['little_pricey','pricey','bit_pricey','overpriced','high_price',\n",
    "                     'way_overpriced','higher_price','premium_price'])\n",
    "    \n",
    "    word_set_good = ['great_price','good_price','low_price','excellent_price',\n",
    "                    'best_price','cheap_price','affordable_price','awesome_price',\n",
    "                    'well_priced','amazing_price','discounted_price','fantastic_price',\n",
    "                    'priced_well','bargain_price','perfect_price','super_price',\n",
    "                    'great_prices','unbeatable_price','inexpensive_price','lowest_price',\n",
    "                    'incredible_price','terrific_price','wonderful_price','great_value',\n",
    "                    'good_value','excellent_value','best_value','better_value','fantastic_value',\n",
    "                    'amazing_value','outstanding_value','incredible_value','awesome_value']\n",
    "    score = 0 \n",
    "    for token in text:\n",
    "        if token in word_set_good:\n",
    "            score = score + 1\n",
    "\n",
    "        if token in word_set_bad:\n",
    "            score = score - 1\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_quality_score(x):\n",
    "    if x > 2:\n",
    "        return 5\n",
    "    elif x == 2:\n",
    "        return 4.0\n",
    "    elif x == 1:\n",
    "        return 3.5\n",
    "    elif x == 0:\n",
    "        return 2.5\n",
    "    elif x == -1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_price_score(x):\n",
    "    if x > 2:\n",
    "        return 5\n",
    "    elif x == 2:\n",
    "        return 4.5\n",
    "    elif x == 1:\n",
    "        return 3.5\n",
    "    elif x == 0:\n",
    "        return 2.5\n",
    "    elif x == -1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_master_index(reviews_file_path, product_dict, reviews_bigram_model, chunk_size):\n",
    "    '''\n",
    "    Constructs a dictionary where the key is a tuple of (review_index, keyword_index) and value is the keyword frequency and query converted to LSI space. This dictionary can then be converted to a sparse matrix.\n",
    "    \n",
    "    Args:\n",
    "        reviews_file_path: file path to the Amazon review json file\n",
    "        product_dict: dictionary that, at the minimum, maps asin's to product categories.\n",
    "        keyword_list: list of the top n keywords\n",
    "        lsi_dictionary: pre-trained LSI dictionary used to convert the reviewText into bag-of-words\n",
    "        lsi_model: pre-trained LSI model to convert the BOW into LSI space\n",
    "        chunk_size: size of the chunks to iterate through\n",
    "\n",
    "    Returns:\n",
    "        user_keyword_dict: dictionary that maps the (review_index, keyword_index) tuple to keyword frequencies\n",
    "        master_index: dictionary that maps review index to asin, reviewerID, and product category\n",
    "    '''\n",
    "\n",
    "    reviews_iter_df = pd.read_json(reviews_file_path, lines=True, chunksize=chunk_size)\n",
    "\n",
    "    master_index = dict() # links index of user_keyword_dict to asin, reviewerID, categoryID\n",
    "    idx_tracker = 0 # since we're chunking, the index will reset to 0 each time we do iterrows(), so we track\n",
    "\n",
    "    # initialize Rake\n",
    "    r = Rake()\n",
    "    \n",
    "    for chunk in tqdm(reviews_iter_df, total=int(math.ceil(6739590/chunk_size))):\n",
    "        for idx, row in chunk.iterrows():\n",
    "\n",
    "            # interestingly, not all asin's in the reviews exist in the products\n",
    "            # if the asin doesn't exist in products, ignore it\n",
    "            if row['asin'] in product_dict:\n",
    "                reviewer_id = row['reviewerID']\n",
    "                asin = row['asin']\n",
    "                overall = row['overall']\n",
    "                category = product_dict[asin]['category']\n",
    "                review_text = row['reviewText']\n",
    "\n",
    "                if pd.isna(review_text) == False:\n",
    "                    bigram_list = list()\n",
    "\n",
    "                    # clean up review_text before applying bigram model\n",
    "                    review_text = review_text.lower()\n",
    "                    tokenized_review_text = word_tokenize(review_text)\n",
    "                    tokenized_review_text = [token for token in tokenized_review_text if not token.isnumeric()] # remove numbers\n",
    "                    tokenized_review_text = [token for token in tokenized_review_text if len(token) > 1] # remove single letter tokens\n",
    "                    stop_words = set(stopwords.words('english'))\n",
    "                    cleaned_review_text = [token for token in tokenized_review_text if token not in stop_words] # remove stop words\n",
    "\n",
    "                    bigrammed_text = reviews_bigram_model[cleaned_review_text]\n",
    "                    for token in bigrammed_text:\n",
    "                        if '_' in token:\n",
    "                            bigram_list.append(token)\n",
    "                    quality_score = transform_quality_score(quality_text_score(bigram_list))\n",
    "                    value_score = transform_price_score(price_text_score(bigram_list))\n",
    "                else:\n",
    "                    quality_score = 0\n",
    "                    value_score = 0\n",
    "\n",
    "                # build the attributes of each review\n",
    "                attr_dict = {'reviewerID' : reviewer_id, 'asin' : asin, 'category' : category, 'overall' : overall, 'quality_score' : quality_score, 'value_score' : value_score}\n",
    "                \n",
    "                if idx_tracker not in master_index:\n",
    "                    master_index[idx_tracker] = attr_dict\n",
    "                else:\n",
    "                    master_index[idx_tracker].update(attr_dict)\n",
    "                \n",
    "                # increment idx_tracker\n",
    "                idx_tracker += 1\n",
    "        \n",
    "    return master_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49540a0bb8c249b0bff95ef93dd58c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# takes 72 minutes\n",
    "master_index = create_master_index('../data/recommend/reviews_with_sentiment_scores_line_delimited.json', product_dict=product_dict, reviews_bigram_model=reviews_bigram_model, chunk_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/recommend/v2/user_keyword_master_index.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(master_index, '../data/recommend/v2/user_keyword_master_index.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create user-keyword dictionary\n",
    "\n",
    "This dictionary maps a tuple of (review_index, keyword_index) to the number of times that keyword showed up in the review. This will then be passed to create a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reviews_keyword_lsi_and_keywords(reviews_file_path, product_dict, keyword_list, lsi_dictionary, lsi_model, lsi_bigram_model, num_rows, num_cols, chunk_size):\n",
    "    '''\n",
    "    Constructs two dictionaries where:\n",
    "    1. One where the key is a tuple of (review_index, keyword_index) and value is the keyword frequency and query converted to LSI space. This dictionary can then be converted to a sparse matrix.\n",
    "    2. One maps the review index to asin, reviewerID, product category, and overall stars.\n",
    "\n",
    "    Args:\n",
    "        reviews_file_path: file path to the Amazon review json file\n",
    "        product_dict: dictionary that, at the minimum, maps asin's to product categories.\n",
    "        keyword_list: list of the top n keywords\n",
    "        lsi_dictionary: pre-trained LSI dictionary used to convert the reviewText into bag-of-words\n",
    "        lsi_model: pre-trained LSI model to convert the BOW into LSI space\n",
    "        chunk_size: size of the chunks to iterate through\n",
    "\n",
    "    Returns:\n",
    "        user_keyword_dict: dictionary that maps the (review_index, keyword_index) tuple to keyword frequencies\n",
    "        master_index: dictionary that maps review index to asin, reviewerID, and product category\n",
    "    '''\n",
    "\n",
    "    reviews_iter_df = pd.read_json(reviews_file_path, lines=True, chunksize=chunk_size)\n",
    "\n",
    "    user_keyword_dict = dict() # holds the (review_index, keyword_index) tuple and number of times a keyword shows up\n",
    "    # master_index = dict() # links index of user_keyword_dict to asin, reviewerID, categoryID\n",
    "    idx_tracker = 0 # since we're chunking, the index will reset to 0 each time we do iterrows(), so we track\n",
    "\n",
    "    # initialize Rake\n",
    "    r = Rake()\n",
    "\n",
    "    # initialize sparse matrix to fill out\n",
    "    sparse_mat = sp.dok_matrix((num_rows, num_cols))\n",
    "    \n",
    "    for chunk in tqdm(reviews_iter_df, total=int(math.ceil(6739590/chunk_size))):\n",
    "        for idx, row in chunk.iterrows():\n",
    "\n",
    "            # interestingly, not all asin's in the reviews exist in the products\n",
    "            # if the asin doesn't exist in products, ignore it\n",
    "            if row['asin'] in product_dict:\n",
    "                reviewer_id = row['reviewerID']\n",
    "                asin = row['asin']\n",
    "                overall = row['overall']\n",
    "                category = product_dict[asin]['category']\n",
    "                review_text = row['reviewText']\n",
    "                values = np.zeros(len(keyword_list)) # array that'll hold the count of keywords per review\n",
    "                \n",
    "                try:\n",
    "                    # only process reviews that are not empty\n",
    "                    if pd.isna(review_text) == False: \n",
    "                        r.extract_keywords_from_text(review_text)\n",
    "                        extracted_keywords = r.get_ranked_phrases()[:10]\n",
    "\n",
    "                        # build the keyword frequencies per (review-keyword) tuple\n",
    "                        for keyword in extracted_keywords:\n",
    "                            if keyword in keyword_list:\n",
    "\n",
    "                                keyword_index = keyword_list.index(keyword)\n",
    "                                tuple_index = (idx_tracker, keyword_index)\n",
    "                                \n",
    "                                if sparse_mat[idx_tracker, keyword_index] == 0:\n",
    "                                    sparse_mat[idx_tracker, keyword_index] = 1\n",
    "                                else:\n",
    "                                    sparse_mat[idx_tracker, keyword_index] += 1\n",
    "\n",
    "                        # use LSI to transform the reviewText into LSI space\n",
    "                        bigrammed_review_text = convert_reviews_to_tokens(review_text, lsi_bigram_model=lsi_bigram_model, lsi_model=lsi_model, lsi_dictionary=lsi_dictionary)\n",
    "                        vec_bow = lsi_dictionary.doc2bow(bigrammed_review_text)\n",
    "                        vec_lsi = lsi_model[vec_bow]  # convert the query to LSI space\n",
    "\n",
    "                        # since vec_lsi start index at 0...\n",
    "                        for element in vec_lsi:\n",
    "                            sparse_mat[idx_tracker, len(keyword_list) + element[0]] = element[1]\n",
    "                        \n",
    "                except:\n",
    "                    print(reviewer_id, '*****', review_text, pd.isna(review_text))\n",
    "                \n",
    "                # increment idx_tracker\n",
    "                idx_tracker += 1\n",
    "        \n",
    "    #return user_keyword_dict, master_index\n",
    "    return sparse_mat.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6732848\n"
     ]
    }
   ],
   "source": [
    "# print(len(user_keyword_master_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d7ef0f9a0b484ba94a1e9f7da1790c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# takes 120 minutes\n",
    "# review_keyword_sparse_mat_with_lsi_keywords = create_reviews_keyword_lsi_and_keywords('../data/recommend/reviews_with_sentiment_scores_line_delimited.json', product_dict=product_dict, keyword_list=keyword_list, lsi_dictionary=lsi_dictionary, lsi_model=lsi_model, lsi_bigram_model=lsi_bigram_model, num_rows=len(user_keyword_master_index), num_cols=1010, chunk_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6732848x1010 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 80562256 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review_keyword_sparse_mat_with_lsi_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/recommend/v2/user_keyword_sparse_mat_with_lsi_and_keywords.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib.dump(review_keyword_sparse_mat_with_lsi_keywords, '../data/recommend/v2/review_keyword_sparse_mat_with_lsi_and_keywords.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split LSI and keyword sparse matrix into LSI only and keyword only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_keyword_sparse_mat_with_lsi_keywords = joblib.load('../data/recommend/v2/review_keyword_sparse_mat_with_lsi_keywords.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_keyword_sparse_mat_with_keywords_only = review_keyword_sparse_mat_with_lsi_keywords[:, :1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_keyword_sparse_mat_with_lsi_only = review_keyword_sparse_mat_with_lsi_keywords[:, 1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/recommend/v2/user_keyword_sparse_mat_with_lsi_only.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib.dump(review_keyword_sparse_mat_with_keywords_only, '../data/recommend/v2/review_keyword_sparse_mat_with_keywords_only.joblib')\n",
    "# joblib.dump(review_keyword_sparse_mat_with_lsi_only, '../data/recommend/v2/review_keyword_sparse_mat_with_lsi_only.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sparse matrices (DEPRECATED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_matrix(input_dict, num_rows, num_cols):\n",
    "    '''\n",
    "    (LEGACY)\n",
    "    Constructs a csr_matrix from the output of create_sparse_dict function.\n",
    "    It uses the dok_matrix as an intermediate matrix to fill in values before converting to csr_matrix. This is because\n",
    "    incrementally replacing values in a csr_matrix is computationally expensive.\n",
    "\n",
    "    Args:\n",
    "        input_dict: output from create_sparse_dict, or a dictionary of format {(row_index, col_index) : keyword_frequency_value}\n",
    "        num_rows: the number of rows that the resulting matrix should have\n",
    "        num_cols: the number of columns that the resulting matrix should have\n",
    "\n",
    "    Returns:\n",
    "        sparse_mat: csr_matrix with shape (num_rows, num_cols)\n",
    "    '''\n",
    "\n",
    "    sparse_mat = sp.dok_matrix((num_rows, num_cols))\n",
    "\n",
    "    for k, v in tqdm(input_dict.items()):\n",
    "        sparse_mat[k[0], k[1]] = v\n",
    "    \n",
    "    return sparse_mat.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_keyword_sparse_mat = create_sparse_matrix(review_keyword_dict, len(user_keyword_master_index), 1010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/recommend/user_keyword_sparse_mat_lsi_only.joblib']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib.dump(review_keyword_sparse_mat, '../data/recommend/user_keyword_sparse_mat_keyword_lsi.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d17e379ec0346749e37ca13c5ae3a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65586771 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# review_keyword_sparse_mat = create_sparse_matrix(user_keyword_dict, len(user_keyword_master_index), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train bigram model on reviews reviewText column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bigram_model(file_path, chunk_size):\n",
    "    docs = list()\n",
    "    iter_reviews = pd.read_json(file_path, lines=True, chunksize=chunk_size)\n",
    "\n",
    "    for chunk in tqdm(iter_reviews, total=int(math.ceil(6739590/chunk_size))):\n",
    "        chunk['reviewText'] = chunk['reviewText'].fillna(0)\n",
    "        chunk['reviewText'] = chunk['reviewText'].astype('string')\n",
    "        for reviews in chunk.reviewText:\n",
    "            docs.append(reviews)\n",
    "\n",
    "    # lower case\n",
    "    # docs = docs.apply(lambda x: x.lower())\n",
    "    docs = [doc.lower() for doc in docs]\n",
    "    # tokenize\n",
    "    # docs = docs.apply(lambda x: word_tokenize(x))\n",
    "    docs = [word_tokenize(doc) for doc in docs]\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    # docs = docs.apply(lambda x: [token for token in x if not token.isnumeric()])\n",
    "    docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "    # remove stopwords \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    docs = [[token for token in doc if token not in stop_words] for doc in docs]\n",
    "    # Compute bigrams.\n",
    "    from gensim.models import Phrases\n",
    "\n",
    "    # Add bigrams\n",
    "    bigram = Phrases(docs, min_count=1)\n",
    "\n",
    "    return bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes 100 minutes to train\n",
    "# reviews_bigram_model = train_bigram_model('../data/Electronics_5.json', chunk_size=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create reviewer-product value matrix\n",
    "\n",
    "Create 4 of these matrices, one for top value, top seller, top quality, and top ratings.\n",
    "Each matrix is structured where each row is a user and each column is a product.\n",
    "\n",
    "Note that even though there are 6.7 million reviews, there are only around 700,000 unique reviewers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_product_mat(review_keyword_master_index, product_index, reviewer_index, mat_val = 'top_value'):\n",
    "    '''\n",
    "    Creates a sparse matrix where rows are users/reviews and columns are asin/products. The values are determined by input mat_val.\n",
    "\n",
    "    Args:\n",
    "        product_dict: product dictionary where keys are asin's and values are product attributes (i.e. price, category, num_features)\n",
    "        user_keyword_master_index: dictionary that maps review index to review attributes (i.e. reviewerID, asin, product category, stars)\n",
    "        mat_val: determines the type of value in the matrix, must choose among ('top_value', 'top_seller', 'top_quality', 'top_ratings')\n",
    "        asin_index: dictionary with {asin_index: asin} format\n",
    "        reviewers_index: dictionary with {reviewer_index: reviewerID} format\n",
    "\n",
    "    Returns:\n",
    "        user_prod_mat: sparse matrix with rows as reviews and columns as products. Values held by the matrix is determined by mat_val.\n",
    "    '''\n",
    "    # initialize variables\n",
    "    # user_prod_mat = sp.dok_matrix((len(reviewers_index), len(asin_index)))\n",
    "    data = list()\n",
    "    row = list()\n",
    "    col = list()\n",
    "    # product_set = set(product_index.values()) # set search is O(1) while list search is O(n)\n",
    "    \n",
    "    if mat_val == 'top_value':\n",
    "        for review_id, review_attr in tqdm(review_keyword_master_index.items()):\n",
    "            asin = review_attr['asin']\n",
    "            reviewer_id = review_attr['reviewerID']\n",
    "            if asin in product_index:\n",
    "                try:\n",
    "                    val = review_attr['value_score']\n",
    "                    # val = product_dict[asin]['price']\n",
    "                    # matching_asin_index = [k for k, v in product_index.items() if v == asin][0]\n",
    "                    # matching_reviewer_index = [k for k, v in reviewer_index.items() if v == reviewer_id][0]\n",
    "                    # user_prod_mat[[matching_reviewer_index], [matching_asin_index]] = val\n",
    "                    matching_product_index = product_index[asin]\n",
    "                    matching_reviewer_index = reviewer_index[reviewer_id]\n",
    "                    \n",
    "                    data.append(val)\n",
    "                    row.append(matching_reviewer_index)\n",
    "                    col.append(matching_product_index)\n",
    "                except:\n",
    "                    print('val:', val, ' asin:', asin)\n",
    "    \n",
    "    elif mat_val == 'top_ratings':\n",
    "        for review_id, review_attr in tqdm(review_keyword_master_index.items()):\n",
    "            asin = review_attr['asin']\n",
    "            reviewer_id = review_attr['reviewerID']\n",
    "            if asin in product_index:\n",
    "                try:\n",
    "                    val = review_attr['overall']\n",
    "                    # matching_asin_index = [k for k, v in asin_index.items() if v == asin][0]\n",
    "                    # matching_reviewer_index = [k for k, v in reviewers_index.items() if v == reviewer_id][0]\n",
    "                    # user_prod_mat[[matching_reviewer_index], [matching_asin_index]] = val\n",
    "                    matching_product_index = product_index[asin]\n",
    "                    matching_reviewer_index = reviewer_index[reviewer_id]\n",
    "                    \n",
    "                    data.append(val)\n",
    "                    row.append(matching_reviewer_index)\n",
    "                    col.append(matching_product_index)\n",
    "                except:\n",
    "                    print('val:', val, ' asin:', asin)\n",
    "\n",
    "    elif mat_val == 'top_seller':\n",
    "        user_product_tracker = dict() # format is {(row, col) : data_index}\n",
    "\n",
    "        for review_id, review_attr in tqdm(review_keyword_master_index.items()):\n",
    "            asin = review_attr['asin']\n",
    "            reviewer_id = review_attr['reviewerID']\n",
    "            if asin in product_index:\n",
    "                try:\n",
    "                    # matching_asin_index = [k for k, v in asin_index.items() if v == asin][0]\n",
    "                    # matching_reviewer_index = [k for k, v in reviewers_index.items() if v == reviewer_id][0]\n",
    "                    # user_prod_mat[[matching_reviewer_index], [matching_asin_index]] += 1\n",
    "                    matching_product_index = product_index[asin]\n",
    "                    matching_reviewer_index = reviewer_index[reviewer_id]\n",
    "\n",
    "                    if (matching_reviewer_index, matching_product_index) in user_product_tracker:\n",
    "                        idx = user_product_tracker[(matching_reviewer_index, matching_product_index)]\n",
    "                        data[idx] += 1\n",
    "                    else:\n",
    "                        user_product_tracker[(matching_reviewer_index, matching_product_index)] = len(data)\n",
    "                        data.append(1)\n",
    "                        row.append(matching_reviewer_index)\n",
    "                        col.append(matching_product_index)\n",
    "                except:\n",
    "                    print('val:', val, ' asin:', asin)\n",
    "    \n",
    "    elif mat_val == 'top_quality':\n",
    "        for review_id, review_attr in tqdm(review_keyword_master_index.items()):\n",
    "            asin = review_attr['asin']\n",
    "            reviewer_id = review_attr['reviewerID']\n",
    "            if asin in product_index:\n",
    "                try:\n",
    "                    val = review_attr['quality_score']\n",
    "                    # matching_asin_index = [k for k, v in asin_index.items() if v == asin][0]\n",
    "                    # matching_reviewer_index = [k for k, v in reviewers_index.items() if v == reviewer_id][0]\n",
    "                    # user_prod_mat[[matching_reviewer_index], [matching_asin_index]] = val\n",
    "                    matching_product_index = product_index[asin]\n",
    "                    matching_reviewer_index = reviewer_index[reviewer_id]\n",
    "                    \n",
    "                    data.append(val)\n",
    "                    row.append(matching_reviewer_index)\n",
    "                    col.append(matching_product_index)\n",
    "                except:\n",
    "                    print('val:', val, ' asin:', asin)\n",
    "    \n",
    "    else:\n",
    "        print('please pass one of the following to mat_val: {\\'top_value\\', \\'top_ratings\\', \\'top_seller\\', \\'top_quality\\'')\n",
    "    \n",
    "    user_prod_mat = sp.coo_matrix((data, (row, col)), shape=(len(reviewer_index), len(product_index)))\n",
    "    \n",
    "    return user_prod_mat.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac02521c05cf4f31a43d767d1493eb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6732848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_product_value_mat = create_user_product_mat(review_keyword_master_index=review_keyword_master_index, product_index=product_index, reviewer_index=reviewer_index, mat_val='top_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca6218543da43539e2bf93daa830d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6732848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_product_ratings_mat = create_user_product_mat(review_keyword_master_index=review_keyword_master_index, product_index=product_index, reviewer_index=reviewer_index, mat_val='top_ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6156f66feb064695b35cdb4db378f6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6732848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_product_seller_mat = create_user_product_mat(review_keyword_master_index=review_keyword_master_index, product_index=product_index, reviewer_index=reviewer_index, mat_val='top_seller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e8bb23062f499591a7aae57474c9b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6732848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_product_quality_mat = create_user_product_mat(review_keyword_master_index=review_keyword_master_index, product_index=product_index, reviewer_index=reviewer_index, mat_val='top_quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<728719x748122 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6481375 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_product_value_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/recommend/v2/user_product_quality_mat.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(user_product_value_mat, '../data/recommend/v2/user_product_value_mat.joblib')\n",
    "joblib.dump(user_product_ratings_mat, '../data/recommend/v2/user_product_ratings_mat.joblib')\n",
    "joblib.dump(user_product_seller_mat, '../data/recommend/v2/user_product_seller_mat.joblib')\n",
    "joblib.dump(user_product_quality_mat, '../data/recommend/v2/user_product_quality_mat.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
