{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98f9674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fredg\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\fredg\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\fredg\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import boto3\n",
    "import random\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import joblib\n",
    "import gzip\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import collections\n",
    "from collections import Counter\n",
    "import re as regex\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
    "# from gensim import utils\n",
    "from bs4 import BeautifulSoup as BSHTML\n",
    "from gensim.parsing.preprocessing import preprocess_string, STOPWORDS #, remove_stopword_tokens\n",
    "import boto3\n",
    "import logging\n",
    "# from ppring import ppring\n",
    "from botocore.exceptions import ClientError\n",
    "import requests\n",
    "from PIL import Image\n",
    "import sys # to access the system\n",
    "import cv2\n",
    "from IPython.display import Image "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc995926",
   "metadata": {},
   "source": [
    "### M2 LAMBDA DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69874c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'index/M2R1_asin_category.pickle'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INPUT FILES:\n",
    "\"datasets/M2_tfidf_mtx\"     \n",
    "'models/M2_tfidf.joblib'\n",
    "'index/M2_category_labels.data'\n",
    "'datasets/M2_rank.data'\n",
    "'index/M2_category_index.pickle'\n",
    "'index/M2R1_asin_category.pickle'\n",
    "\n",
    "# OUTPUT: top 5 category labels and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67fd1f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Lambda function: takes a query and returns the top 5 category keys of recommendations\n",
    "INPUT files: \"M2_tfidf_mtx.csv\" , \"M2.joblib\" ,  \"M2_categories.json\" + query\n",
    "OUTPUT: top 5 tuples of matching categoris\n",
    "1- Creates a class tfidf and download dependencies in the first instance, tries to save in cache\n",
    "2- Takes each query and vectorize\n",
    "3- Calculate simirality tuples\n",
    "4- Ranks recommendation based on similarity and np.log(count of asin in each category) '''\n",
    "\n",
    "# match=set(self.M2.get_feature_names_out()) & set(liste)\n",
    "# creating class to import necessary matrices and models for retrival\n",
    "class tfidf:\n",
    "    def __init__(self):\n",
    "        # load cleaned group_by data sets and models from disk\n",
    "        self.mtx=self.load_sparse_csr(\"datasets/M2_tfidf_mtx\")        \n",
    "        self.M2 = joblib.load('models/M2_tfidf.joblib')\n",
    "        self.stemmer = PorterStemmer() \n",
    "        self.quer_R1 = None\n",
    "        self.simi = None\n",
    "        \n",
    "        with open('index/M2R1_big_dico.pickle', 'rb') as filehandle:\n",
    "            self.big_index = pickle.load(filehandle)\n",
    "        \n",
    "        self.tfidf_mtx = self.load_sparse_csr(\"datasets/R1_tfidf_mtx\")\n",
    "        self.model = joblib.load('models/R1_tfidf.joblib')\n",
    "            \n",
    "            \n",
    "        with open('index/M2_category_index.pickle', 'rb') as filehandle:\n",
    "            self.category_index = pickle.load(filehandle)\n",
    "            \n",
    "        with open('index/M2R1_asin_category.pickle', 'rb') as filehandle:\n",
    "            self.asin_category = pickle.load(filehandle)\n",
    "\n",
    "            \n",
    "    def load_sparse_csr(self, filename):\n",
    "        # here we need to add .npz extension manually\n",
    "        loader = np.load(filename + '.npz')\n",
    "        return csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape']) \n",
    "    \n",
    "    def asin_of_categories(self,cat_idx):\n",
    "        \n",
    "        info = self.big_index[cat_idx]\n",
    "        cat = info[0]\n",
    "        start = info[1]\n",
    "        finish = info[2]\n",
    "        try:\n",
    "            simi_mtx = self.simi[start:finish,]\n",
    "        except:\n",
    "            self.simi = cosine_similarity(self.tfidf_mtx,self.quer_R1)\n",
    "            simi_mtx = self.simi[start:finish,]\n",
    "            \n",
    "        arr2 = np.argsort(-simi_mtx,axis=0)[:5] + start\n",
    "        dim = arr2.shape[0]\n",
    "        idx_asin = list(arr2.reshape(dim,)) \n",
    "        \n",
    "        return idx_asin\n",
    "        \n",
    "        \n",
    "    \n",
    "    ## retrieve match tfidf (LAMBDA FUNCTION)\n",
    "    def M2_query_to_category_function(self,q):\n",
    "        stop_words= set(stopwords.words('english'))\n",
    "        liste=[self.stemmer.stem(w) for w in word_tokenize(q) if not w.lower() in stop_words]\n",
    "        n_valid_words= len([x for x in liste if len(x)>2])\n",
    "        string=' '.join(element for element in liste)       \n",
    "        quer = self.M2.transform([string])\n",
    "        self.quer_R1 = self.model.transform([string])\n",
    "        \n",
    "        simi = cosine_similarity(self.mtx,quer) \n",
    "        \n",
    "        Beta_factor = 0.0\n",
    "        simi_mtx = simi # + self.rank * Beta_factor # rank is a coeff that you can tweak np.log(gb[\"count\"] + 1) * 0.35\n",
    "        \n",
    "        # results = list(self.category[np.argsort(-simi_mtx,axis=0)[:5].reshape(5,)])\n",
    "        idx= list(np.argsort(-simi_mtx,axis=0)[:5].reshape(5,))\n",
    "        \n",
    "        liste = []\n",
    "        for element in idx:\n",
    "            ##### M2 returns indices\n",
    "            retour = self.asin_of_categories(element)\n",
    "            liste = liste + retour\n",
    "        \n",
    "        return idx, np.round(np.max(simi),2), n_valid_words, liste\n",
    "    \n",
    "    ## testing R1 based on threshold\n",
    "    def R1_query_to_category(self):\n",
    "        simi = cosine_similarity(self.tfidf_mtx,self.quer_R1)\n",
    "        self.simi = simi\n",
    "        idx_asin = list(np.argsort(-simi,axis=0)[:5].reshape(5,))\n",
    "        idx_category = [ self.asin_category[x] for x in idx_asin]\n",
    "        \n",
    "        # returns index_list of ASIN\n",
    "        liste = []\n",
    "        for element in idx_category:\n",
    "            ## liste of ASIN indices\n",
    "            retour = self.asin_of_categories(element)\n",
    "            liste = liste + retour\n",
    "        \n",
    "        return idx_category, np.round(np.max(simi),2), liste\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93e5157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# take as input the 1000 query test data frame and returns the metrics in an array\n",
    "def testing_queries(min_query,max_query, query_test,threshold_list): \n",
    "    # calling the class\n",
    "    ob = tfidf() \n",
    "    spread = max_query-min_query\n",
    "    arr = np.zeros((spread * len(threshold_list)*2, 13))\n",
    "    for n_words in range(0,spread):    \n",
    "        valid = 0\n",
    "        col = \"query_{}\".format(n_words + min_query)\n",
    "        queries = list(zip(query_test[col], query_test.cat_idx, query_test.label, query_test.asin))\n",
    "\n",
    "        for element in queries:\n",
    "\n",
    "            q = element[0]\n",
    "            cat_idx = element[1]\n",
    "            label = element[2]\n",
    "            asin_idx = element[3]\n",
    "\n",
    "            liste_idx, first_maxi, n_valid_words, liste_asin_first = ob.M2_query_to_category_function(q)\n",
    "\n",
    "\n",
    "            valid += n_valid_words\n",
    "            idx_category, second_maxi, liste_asin_second = ob.R1_query_to_category()\n",
    "\n",
    "            # for matches in range(1,len(idx)):\n",
    "            first = liste_idx #[:matches]\n",
    "            second = idx_category #[:matches]\n",
    "\n",
    "            first_match = len(set(first) & set([cat_idx]))\n",
    "            second_match = len(set(second) & set([cat_idx]))\n",
    "\n",
    "            matches_asin = len(set([asin_idx]) & set(liste_asin_first)) + len(set([asin_idx]) & set(liste_asin_second))\n",
    "\n",
    "            for top_n in range(0,2):\n",
    "                if top_n == 1:\n",
    "                    first = liste_idx[:3]\n",
    "                    second = idx_category[:3]\n",
    "\n",
    "                    first_match = len(set(first) & set([cat_idx]))\n",
    "                    second_match = len(set(second) & set([cat_idx]))\n",
    "\n",
    "\n",
    "                for j, threshold in enumerate(threshold_list):\n",
    "                    indice = j + n_words * len(threshold_list) + top_n * (spread * len(threshold_list))\n",
    "                    # if label == 1\n",
    "                    if label:\n",
    "                        if (first_maxi > threshold and first_match) or (second_maxi > threshold and second_match):\n",
    "                            # counting TP and matches\n",
    "                            arr[indice,0] += 1\n",
    "                            arr[indice,1] += 1\n",
    "                            ### at asin level\n",
    "                            arr[indice,9] += matches_asin\n",
    "                        else:\n",
    "                            # counting FN\n",
    "                            arr[indice,3] += 1\n",
    "                            arr[indice,10] += 1\n",
    "                    # if label == 0\n",
    "                    else:\n",
    "                        # counting FP and matches\n",
    "                        if (first_maxi > threshold) or (second_maxi > threshold):\n",
    "                            arr[indice,0] += 1\n",
    "                            arr[indice,2] += 1\n",
    "                            arr[indice,11] += 1\n",
    "                        # counting TN \n",
    "                        else:\n",
    "                            arr[indice,4] += first_match\n",
    "                            arr[indice,12] += 1\n",
    "                    # counting real stem words\n",
    "                    arr[indice,5] = np.round(valid / len(queries),2)\n",
    "                    arr[indice,6] = threshold\n",
    "                    arr[indice,7] = n_words + min_query\n",
    "                    arr[indice,8] = len(first)\n",
    "    return arr\n",
    "\n",
    "# takes the array of metrics and returns the formated data frame with all the metrics and the merged\n",
    "def formating_array(arr, filename, longueur):  \n",
    "    col=[\"match\",\"TP\",\"FP\",\"FN\",\"TN\",\"stem_words\",\"threshold\",\"n_words\",\"TOP_N\",\"TP_asin\",\"FN_asin\", \"FP_asin\",\"TN_asin\"]  \n",
    "    df = pd.DataFrame(arr, columns = col)\n",
    "    df[\"queries\"] = longueur\n",
    "    df[\"TN\"] = longueur - (df[\"TP\"] + df[\"FP\"] + df[\"FN\"])\n",
    "    df= df [['n_words','stem_words',\"threshold\",\"TOP_N\",'queries','match', 'TP','FP', 'FN', 'TN',\"TP_asin\",\"FN_asin\", \"FP_asin\",\"TN_asin\"]]\n",
    "    df[\"accuracy\"] = np.round((df[\"TP\"] + df[\"TN\"]) / longueur, 2) # check\n",
    "    df[\"Precision\"] = np.round(df[\"TP\"]/ (df[\"TP\"] + df[\"FP\"]),2) # check\n",
    "    df[\"Recall\"] = np.round(df[\"TP\"]/ (df[\"TP\"] + df[\"FN\"]), 2) #check\n",
    "\n",
    "    df[\"FN_asin\"] = df[\"FN_asin\"] + df[\"TP_asin\"] - df[\"TP_asin\"]\n",
    "    df[\"accuracy_asin\"] = np.round((df[\"TP_asin\"] + df[\"TN_asin\"]) / longueur, 2) # check\n",
    "    df[\"Precision_asin\"] = np.round(df[\"TP_asin\"]/ (df[\"TP_asin\"] + df[\"FP_asin\"]),2) # check\n",
    "    df[\"Recall_asin\"] = np.round(df[\"TP_asin\"]/ (df[\"TP_asin\"] + df[\"FN_asin\"]), 2) \n",
    "    df[\"F1\"] = np.round((2 * df[\"Precision\"] * df[\"Recall\"]) / (df[\"Precision\"] + df[\"Recall\"]),2)\n",
    "    df[\"F1_asin\"] = np.round((2 * df[\"Precision_asin\"] * df[\"Recall_asin\"]) / (df[\"Precision_asin\"] + df[\"Recall_asin\"]),2)\n",
    "    \n",
    "    # export to csv file\n",
    "    df1 = df.reset_index()\n",
    "    df1 = df1[[\"index\",\"threshold\",\"accuracy\",\"Precision\",\"Recall\",\"F1\",\"F1_asin\"]]\n",
    "    df1.columns = [\"idx\",\"threshold\",\"accuracy\",\"Precision\",\"Recall\",\"F1\",\"F1_asin\"]\n",
    "        \n",
    "    return df \n",
    "\n",
    "# Takes a data frame and returns \n",
    "def top_results(metrics,data):\n",
    "    data = df_results\n",
    "    df1 = data.reset_index()\n",
    "    df1 = df1[[\"index\",\"threshold\",\"accuracy\",\"Precision\",\"Recall\",\"F1\",\"Precision_asin\",\"F1_asin\"]]\n",
    "    df1.columns = [\"idx\",\"threshold\",\"accuracy\",\"Precision\",\"Recall\",\"F1\",\"Precision_asin\",\"F1_asin\"]\n",
    "    col = metrics  \n",
    "    machin = data.groupby(['n_words','top_n'])[col].nlargest(1).reset_index()\n",
    "    del machin[col]\n",
    "    machin.columns = [\"n_words\",\"top_n\",\"idx\"]\n",
    "    df_merge = machin.merge(df1, on = \"idx\", suffixes=(False, False))\n",
    "    df_merge[\"n_words\"] = df_merge[\"top_n\"].astype(int)\n",
    "    df_merge[\"top_n\"] = df_merge[\"top_n\"].astype(int)\n",
    "    del df_merge[\"idx\"]\n",
    "    \n",
    "    return df_merge\n",
    "\n",
    "\n",
    "def initialize():\n",
    "    # range of length of query words to test [start, finish[ \n",
    "    min_query = 3\n",
    "    max_query = 8\n",
    "\n",
    "    ############# KEEP TRUE AS DEFAULT ##############\n",
    "    download = True\n",
    "    ############################################\n",
    "    \n",
    "    # will dowload the filename results\n",
    "    if download:\n",
    "        filename = \"results/M2+R2_full_results.csv\" \n",
    "        df_results = pd.read_csv(filename)  \n",
    "    \n",
    "    # re-runs everything based on importing the query_test dataset\n",
    "    if not download:       \n",
    "        filename = \"datasets/query_test_1000.csv\"\n",
    "        query_test = pd.read_csv(filename) \n",
    "        # Setting the similarity threshold range\n",
    "        threshold_list = [x/100 for x in range(0,101)]\n",
    "        # threshold_list = [0.30,0.4,0.45,0.50,0.55,0.6]\n",
    "        # query_test = query_test.sample(100)\n",
    "        ## returns the array with all the metrics, longuest query\n",
    "        arr = testing_queries(min_query,max_query, query_test,threshold_list)\n",
    "        longueur = len(query_test)\n",
    "        df_results = formating_array(arr, filename, longueur)\n",
    "        filename = \"results/M2+R2_full_results.csv\"\n",
    "        df_results.to_csv(filename, index=False)\n",
    "    \n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53f2095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "df_results = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cb70146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_words</th>\n",
       "      <th>stem_words</th>\n",
       "      <th>threshold</th>\n",
       "      <th>TOP_N</th>\n",
       "      <th>queries</th>\n",
       "      <th>match</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>...</th>\n",
       "      <th>FP_asin</th>\n",
       "      <th>TN_asin</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>accuracy_asin</th>\n",
       "      <th>Precision_asin</th>\n",
       "      <th>Recall_asin</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1_asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>847.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>489.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.53</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>847.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>489.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.53</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>847.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>489.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.53</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>847.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>489.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.53</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>847.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>489.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>7.0</td>\n",
       "      <td>5.54</td>\n",
       "      <td>0.96</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>79.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>421.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>7.0</td>\n",
       "      <td>5.54</td>\n",
       "      <td>0.97</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>427.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>7.0</td>\n",
       "      <td>5.54</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>431.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>7.0</td>\n",
       "      <td>5.54</td>\n",
       "      <td>0.99</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>67.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>7.0</td>\n",
       "      <td>5.54</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1010 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      n_words  stem_words  threshold  TOP_N  queries  match     TP     FP  \\\n",
       "0         3.0        2.53       0.00    5.0     1000  847.0  358.0  489.0   \n",
       "1         3.0        2.53       0.01    5.0     1000  847.0  358.0  489.0   \n",
       "2         3.0        2.53       0.02    5.0     1000  847.0  358.0  489.0   \n",
       "3         3.0        2.53       0.03    5.0     1000  847.0  358.0  489.0   \n",
       "4         3.0        2.53       0.04    5.0     1000  847.0  358.0  489.0   \n",
       "...       ...         ...        ...    ...      ...    ...    ...    ...   \n",
       "1005      7.0        5.54       0.96    3.0     1000   79.0   79.0    0.0   \n",
       "1006      7.0        5.54       0.97    3.0     1000   73.0   73.0    0.0   \n",
       "1007      7.0        5.54       0.98    3.0     1000   69.0   69.0    0.0   \n",
       "1008      7.0        5.54       0.99    3.0     1000   67.0   67.0    0.0   \n",
       "1009      7.0        5.54       1.00    3.0     1000    0.0    0.0    0.0   \n",
       "\n",
       "         FN     TN  ...  FP_asin  TN_asin  accuracy  Precision  Recall  \\\n",
       "0     142.0   11.0  ...    489.0     11.0      0.37       0.42    0.72   \n",
       "1     142.0   11.0  ...    489.0     11.0      0.37       0.42    0.72   \n",
       "2     142.0   11.0  ...    489.0     11.0      0.37       0.42    0.72   \n",
       "3     142.0   11.0  ...    489.0     11.0      0.37       0.42    0.72   \n",
       "4     142.0   11.0  ...    489.0     11.0      0.37       0.42    0.72   \n",
       "...     ...    ...  ...      ...      ...       ...        ...     ...   \n",
       "1005  421.0  500.0  ...      0.0    500.0      0.58       1.00    0.16   \n",
       "1006  427.0  500.0  ...      0.0    500.0      0.57       1.00    0.15   \n",
       "1007  431.0  500.0  ...      0.0    500.0      0.57       1.00    0.14   \n",
       "1008  433.0  500.0  ...      0.0    500.0      0.57       1.00    0.13   \n",
       "1009  500.0  500.0  ...      0.0    500.0      0.50        NaN    0.00   \n",
       "\n",
       "      accuracy_asin  Precision_asin  Recall_asin    F1  F1_asin  \n",
       "0              0.25            0.33         0.63  0.53     0.43  \n",
       "1              0.25            0.33         0.63  0.53     0.43  \n",
       "2              0.25            0.33         0.63  0.53     0.43  \n",
       "3              0.25            0.33         0.63  0.53     0.43  \n",
       "4              0.25            0.33         0.63  0.53     0.43  \n",
       "...             ...             ...          ...   ...      ...  \n",
       "1005           0.58            1.00         0.16  0.28     0.28  \n",
       "1006           0.57            1.00         0.15  0.26     0.26  \n",
       "1007           0.57            1.00         0.14  0.25     0.25  \n",
       "1008           0.57            1.00         0.13  0.23     0.23  \n",
       "1009           0.50             NaN         0.00   NaN      NaN  \n",
       "\n",
       "[1010 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d191a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_words</th>\n",
       "      <th>top_n</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision_asin</th>\n",
       "      <th>F1_asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_words  top_n  threshold  accuracy  Precision  Recall    F1  \\\n",
       "0        3      3       0.49      0.51       0.51    0.57  0.54   \n",
       "1        5      5       0.49      0.54       0.54    0.64  0.59   \n",
       "2        3      3       0.53      0.67       0.68    0.64  0.66   \n",
       "3        5      5       0.52      0.68       0.67    0.69  0.68   \n",
       "4        3      3       0.55      0.77       0.81    0.71  0.76   \n",
       "5        5      5       0.51      0.76       0.75    0.79  0.77   \n",
       "6        3      3       0.52      0.82       0.82    0.82  0.82   \n",
       "7        5      5       0.50      0.83       0.80    0.88  0.84   \n",
       "8        3      3       0.54      0.86       0.88    0.84  0.86   \n",
       "9        5      5       0.52      0.86       0.85    0.87  0.86   \n",
       "\n",
       "   Precision_asin  F1_asin  \n",
       "0            0.42     0.45  \n",
       "1            0.45     0.50  \n",
       "2            0.64     0.62  \n",
       "3            0.63     0.64  \n",
       "4            0.78     0.72  \n",
       "5            0.71     0.73  \n",
       "6            0.81     0.80  \n",
       "7            0.77     0.81  \n",
       "8            0.88     0.85  \n",
       "9            0.84     0.85  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = \"F1\"\n",
    "data = df_results\n",
    "data[\"top_n\"] = data[\"TOP_N\"]\n",
    "summary_results = top_results(metrics,data)\n",
    "summary_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e59a195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
